<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Leetcode题选]]></title>
    <url>%2F2019%2F03%2F17%2Fleetcode%2F</url>
    <content type="text"><![CDATA[数组 283 MoveZeros 27 Remove Element 26 Remove Duplicated from Sorted Array 80 Remove Duplicated from Sorted Array 2 75 Sort Colors 88 Merge Sorted Array 215 Kth Largest Element in an Array 双索引技术 (对撞指针) 167 Two Sum 2 - Input array is sorted 125 Valid Palindrome 344 Reverse String 345 Reverse Vowels of a String 11 Container With Most Water (滑动窗口) 209 Minimum Size Subarray Sum 3 Longest SubString Without Repeating Characters 438 Find All Anagrams in a String 76 Minimum Window Substring 数组题思维导图小结，请用ithoughts打开.对应代码 查找表 349 Intersection of Two Arrays 350 Intersection of Two Arrays 242 Valid Anagram 202 Happy Number 290 Word Pattern 205 Isomorphic Strings 451 Sort Characters By Frequency 1 Two Sum(先排序，再用对撞指针，nlogn)(查找表) 15 3Sum 18 4Sum 16 3Sum Closest(不一定适用查找表) 454 4Sum 2 49 Group Anagrams 447 Number of Boomerangs(整形越界的小陷阱,浮点误差) 148 Max Points on a Line (滑动窗口+查找表) 219 Container Duplicate 2 217 Container Duplicate 220 Container Duplicate 3(tree set) 链表 206 Reverse Linked List 92 Reverse Linked List 2 83 Remove Duplicates from Sorted List 86 Partitions List 328 Odd Even Linked List 2 Add Two Numbers(负数?) 445 Add Two Numbers2 (虚拟头结点) 203 Remove Linked List Element 82 Remove Duplicated from Sorted List 2 21 Merge Two Sorted Lists 24 Swap Nodes in Paris 25 Reverse Nodes in k-Group 147 Insertion Sort List 148 Sort List (不仅仅是穿针引线) 237 Delete Node in a Linked List (链表和双指针) 19 Remove Nth Node From End of List 61 Rotate List 143 Reorder List 234 Palindrome Linked List 栈 20 Valid Parentheses 150 Evaluate Reverse Polish Notation 71 Simiplify Path (栈和递归的紧密关系) 144 Binary Tree Preorder Traversal（用stack模拟系统栈实现非递归遍历） 94 Binary Tree Inorder Traversal 145 Binary Tree Postorder Traversal 341 Flatten Nested List Iterator 队列队列的基本应用：广度优先遍历 树；层序遍历 图； 无权图的最短路径 (层序遍历) 102 Binary Tree Level Order Traversal 107 Binary Tree Level Order Traversal2 103 Binary Tree Zigzag Level Order Traversal 199 Binary Tree Right Side View (BFS和图的最短路径) 279 Perfect Squares(贪心算法不成立，转化为图或树) 127 Word Ladder 126 Word Ladder 2 优先队列（堆，白板编程） 347 Top K Frequent Element(nlogk, nlog(n-k)) 23 Merge K Sorted Lists 二叉树和递归 104 Maximum Depth of Binary Tree 111 Minimum Depth of Binary Tree 226 Invert Binary Tree 100 Same Tree 101 Symmetric Tree 222 Count Complete Tree Nodes 110 Balanced Binary Tree （递归终止条件） 112 Path Sum 111 Minimum Depth of Binary Tree 404 Sum of Left Leaves （使用递归函数的返回值） 257 Binary Tree Paths 113 Path Sum2 129 Sum Root to Leaf Numbers 437 Path Sum3 （BST） 235 Lowest Common Ancestor of a Binary Search Tree 98 Validate Binary Search Tree 450 Delete Node in a BST 108 Convert Sorted Array to BST 230 Kth Smallest Element in a BST 236 Lowest Common Ancestor of a Binary Tree(LCA) 递归和回溯（树形问题） 17 Letter Combination of a Phone Number(O(2^N)) 93 Restore IP Address 131 Palindrome Partitioning (排列问题) 46 Permutations (Perms(nums[0…n-1])={取出一个数字}+ Perms(nums[{0..n-1}-这个数字])) 47 Permutations2 (组合问题) 77 Combinations（剪枝） 39 Combination Sum 40 Combination Sum2 216 Combination Sum3 78 Subsets 90 Subsets2 401 Binary Watch (二维平面上使用回溯法) 79 Word Search (偏移量) 200 Number of Islands(floodfill, DFS) 130 Surrounded Regions 417 Pacific Atlantic Water Flow 51 N-Queens 52 N-Queens2 37 Sudoku Solver 动态规划重叠子问题记忆化搜索（递归的基础上添加记忆化过程）- 自上向下的解决问题动态规划 - 自下向上的解决问题 70 Climbing Stairs 120 Triangle 64 Minimum Path Sum(条件) 最优子结构 343 Integer Break（暴力2^N,不知道几重循环，用递归,组合的解空间）1 279 Perfect Squares 91 Decode Ways 62 Unique Paths 63 Unique Paths2 （状态和状态转移） 198 House Robber(暴力2^n)（状态和状态转移，两种状态定义） 213 House Robber2 337 House Robber3 309 Best Time to Buy and Sell Stock with Cooldown （0-1 背包）暴力2^N 416 Partition Equal Subset Sum 322 Coin Change 377 Combination Sum4 474 Ones and Zeroes 139 Word Break 494 Target Sum （LIS） 300 Longest Increasing Subsequence（n^2动态规划，有nlgn） 376 Wiggle Subsequence (最长公共子序列) (dijkstra) (所有最长上升子序列是什么) (0-1背包存放的是什么) 贪心算法（排序） 455 Assign Cookies 392 Is Subsequence (贪心算法和动态规划) 435 Non-overlapping Intervals 贪心选择性质问题的整体最优解可以通过一系列局部最优的选择，即贪心选择来达到。 （最小生成树，最短路径）]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度兴趣网络(DIN,Deep Interest Network)]]></title>
    <url>%2F2018%2F08%2F02%2FDIN%2F</url>
    <content type="text"><![CDATA[Deep Interest Network: 阿里深度兴趣网络摘要深度兴趣网络(DIN,Deep Interest Network):该方法由阿里妈妈的精准定向检索及基础算法团队提出, 针对电子商务领域(e-commerce industry), 充分利用/挖掘用户历史行为数据中的信息来提高CTR预估的性能. contributions/key words: 1234-diversity -local activation (attention) -mini-batch aware regularizer -data adaptive activation function (dice) old methods: 1234567常见的算法, 比如 Wide&amp;Deep, DeepFM等, 通常流程是: Sparse Features -&gt; Embedding Vector (-&gt; pooling layer) -&gt; MLPs -&gt; Sigmoid -&gt; Output 通过Embedding层, 将高维离散特征转换为固定长度的连续特征, 然后通过多个全联接层, 学习非线性, 最后通过一个sigmoid函数转化为0-1值, 代表点击的概率优点：通过神经网络可以拟合高阶的非线性关系, 同时减少了人工特征的工作量. 缺点：在对用户历史行为数据进行处理时, 每个用户的历史点击个数是不相等的, 包含了许多兴趣信息, 我们要把它们编码成一个固定长的向量, 需要做pooling (sum or average), 会损失信息 DIN: 12Diversity：用户在浏览电商网站的过程中显示出的兴趣是十分多样性的. Local activation: 由于用户兴趣的多样性, 只有部分历史数据会影响到当次推荐的物品是否被点击, 而不是所有的历史记录. Features: 12只有用户行为特征是multi-hot, 即多值离散. 没有人工组合特征, 会在dnn中自己学习. Architecture: 12Activation Unit实现Attention机制, 对Local Activation建模. Pooling(weighted sum)对Diversity建模, 直接sum体现不出差异多样性, 加权可以. 123456789其中： Vi表示behavior id的嵌入向量, 比如good_id, shop_id等; Vu是所有behavior ids的加权和, 表示的是用户兴趣; Va是候选广告的嵌入向量; wi是候选广告影响着每个behavior id的权重, 也就是Local Activation; wi通过Activation Unit计算得出, 这一块用函数去拟合, 表示为g(Vi,Va). 在实际实现中, 权重用激活函数Dice的输出来表示, 输入是Vi和Va. 优点：针对不同的候选广告, 用户的兴趣向量是不同的, 而不像单纯的sum pooling兴趣永远是不变的. DICE: data adaptive activation function: 类似relu + BN 的组合: 123456优点: 1.将数据做标准化估计, 统一所有维度的量纲, 也是个非常重要的技巧, 有点类似BN.--在训练过程中, 分别是当次batch的均值和方差.--在Test时, 这里的E[s]和Var[s]用的是moving_average.2.可以发现当E[s]=0, Var[s]=0 时, Dice几乎等同于PRELU.3.ϵ is a small constant which is set to be 10−8 in our practice. GAUC: 计算了用户级别的AUC, 在将其按展示次数进行加权, 消除了用户偏差对模型评价的影响, 更准确地描述了模型对于每个用户的表现效果 1234567891011121314151617AUC意义: AUC值越大, 当前的分类算法越有可能将正样本排在负样本前面, 即能够更好的分类.首先要肯定的是, AUC是要分用户看的, 我们的模型的预测结果, 只要能够保证对每个用户来说, 他想要的结果排在前面就好了.Example 1：假设有两个用户A和B, 每个用户都有10个商品, 10个商品中有5个是正样本.我们分别用TA, TB, FA, FB来表示两个用户的正样本和负样本.假设模型预测的结果大小排序依次为TA, FA, TB, FB1）如果把两个用户的结果混起来看, AUC并不是很高, 因为有5个正样本排在了后面.2）但是分开看的话, 每个用户的正样本都排在了负样本之前, AUC应该是1.显然, 分开看更容易体现模型的效果, 这样消除了用户本身的差异.Example 2：还有一种差异是用户的展示次数或者点击数, 这种差异同样需要消除.如果一个用户有1个正样本, 10个负样本; 另一个用户有5个正样本, 50个负样本.那么GAUC的计算, 不仅将每个用户的AUC分开计算, 同时根据用户的展示数或者点击数来对每个用户的AUC进行加权处理, 进一步消除了用户偏差对模型的影响.通过实验证明, GAUC确实是一个更加合理的评价指标. mini-batch aware Regularization （MBA）: 1234567891011场景:CTR中输入稀疏而且维度高, 通常的做法是加入L1、L2、Dropout等防止过拟合.--但是论文中尝试后效果都不是很好：用户数据符合长尾定律(long-tail law), 也就是说很多的feature id只出现了几次, 而一小部分feature id出现很多次.--这在训练过程中增加了很多噪声, 并且加重了过拟合.对于这个问题一个简单的处理办法就是：--直接去掉出现次数比较少的feature id.但是这样就人为的丢掉了一些信息, 导致模型更加容易过拟合.--同时阈值的设定作为一个新的超参数, 也是需要大量的实验来选择的.MBA的优点:1.频率自适应 2.每次迭代只更新非0部分权重,减少计算量 原理: 效果: 12利用候选的广告, 反向激活历史兴趣, 不同的历史兴趣爱好对于当前候选广告的权重不同, 做到了local activation.--可以看到, 对于候选的广告是一件衣服的时候, 用户历史行为中跟衣服相关性越高的在attention后获得的权重越高（即越能描述用户对这个广告的兴趣）, 而非衣服的部分, 权重较低. 123以上面年轻妈妈为例, 选取9个类别各100条商品, 作为candidate ad输入模型, 得到每件商品的embedding vector以及预测得分.进行可视化如上图, 红色表示得分最高, 蓝色表示得分最低,可以发现：--用户的兴趣分布有多个峰.且DIN有较好的聚类效果. summary: 12345671.用户有多个兴趣爱好, 即访问了多个good_id, shop_id.为了降低维度并使得商品店铺间的算术运算有意义, 我们先对其进行Embedding嵌入.那么我们如何对用户多种多样的兴趣建模？--使用Pooling对Embedding Vector求和或者求平均.同时这也解决了不同用户输入长度不同的问题, 得到了一个固定长度的向量.这个向量就是用户表示, 是用户兴趣的代表.2.但是, 直接求sum或average(相当于平均权重, 没有侧重)损失了很多信息.--所以稍加改进, 针对不同的behavior赋予不同的权重, 这个权重是由用户历史behavior和当前候选广告共同决定的.这就是Attention机制, 实现了Local Activation.3.DIN使用activation unit来捕获local activation的特征, 使用weighted sum pooling来实现diversity结构.4.在模型学习优化上, DIN提出了Dice激活函数、MBA(自适应正则化) , 显著的提升了模型性能与收敛速度. Reference: 123456Github: https://github.com/running-Bad-AI/DeepInterestNetworkDeep Interest Network for Click-Through Rate PredictionLearning piece-wise linear models from large scale data for ad click predictionhttps://www.leiphone.com/news/201707/t0AT4sIgyWS2QWVU.htmlhttps://www.leiphone.com/news/201706/pDfOAoMYp8mqNKEC.html盖坤的分享视频 http://www.itdks.com/dakalive/detail/3166]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐常用排序模型总结]]></title>
    <url>%2F2018%2F08%2F02%2F%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[深度学习在花椒直播中的应用——排序算法篇排序算法概述现代推荐系统一般分为召回和排序两个阶段。召回阶段一般会用一些成本低、速度快的模型从十万、百万量级的候选集中初步筛选，留下千、百个；然后在排序阶段用更加精细的特征和复杂的模型来进行精排，最终留下topK个。 近十年间，业界排序模型的发展可以说是一日千里，从千篇一律的LR，到2010年FM的提出，再到2014年Facebook提出的树模型GBDT，这几年可以看成是现代推荐系统的上半场； 而2015年至今可以看成是飞速发展的下半场，几年之间，以DNN等模型为代表的深度学习网络，如雨后春笋般的出现，各种模型架构、特征交叉方式层出不穷，各种新的idea、trick令人眼花缭乱，而深度学习也逐渐成为了CTR、推荐领域的主流方法。 本文将沿着这一路线，对这些排序模型进行简单的介绍、梳理和总结。 一、传统模型1. LR在深度学习崛起之前，LR以其简单、速度快、可解释性强的优势，几乎垄断了早期的CTR、推荐领域。 $$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}x}}$$ 直观来讲，LR模型目标函数的形式就是各特征的加权和，再通过sigmoid函数将结果映射到0-1之间来表达用户点击某个物品的概率，简单易懂，而且实现时可以并行，速度快。 此外，通过观察学习到的各特征权重，我们可以轻易得知哪些特征比较“重要”，在预测出现偏差的时候，也可以很容易看出哪些因素影响了结果，这就是为什么说它可解释性强的原因。 然而LR的缺点也很明显：由于它是简单的线性模型，就不能处理特征和目标之间的非线性关系，而且特征之间并不是完全独立的，有些特征交叉使用会有特别的效果。因此为了让模型有一定的非线性，当时的数据科学家需要手工做很大量的特征工程，比如连续特征离散化、特征之间的交叉等。但想要提取出高效的交叉特征，需要充分了解数据和场景，人力成本较高，而且再有经验的工程师也难以穷尽所有的特征交叉组合。 既然手工很难，能不能自动寻找特征交叉组合或者借助模型进行呢？后几年中出现了以FM、GBDT为代表的两种自动特征交叉方法。 2. FM/FFM针对LR中特征交叉的问题，有人提出了下面的多项式模型：$$ y(\mathbf{x}) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n w_{ij} x_i x_j $$从公式可以看出，模型对所有特征进行了两两交叉，并对所有的特征组合赋予了权重。 但这种暴力的方法存在着下面两个问题： 现代推荐系统中往往包含大量稀疏的特征（如id类），而交叉特征的维度是原始特征维度的乘积，并且当且仅当两个特征值都为非0时，其对应的组合权重才会被更新到，这会使得大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛，而且权重参数的数量由$n$直接上升到$n^2$，极大增加了训练复杂度。 无法泛化到未曾在训练样本中出现过的特征组合中 针对上面两个问题，2010年德国康斯坦茨大学的Steffen Rendle提出了FM（Factorization Machine）$$ y(\mathbf{x}) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j $$ 类似于矩阵分解（MF）的思路，FM为每个特征学习了一个隐权重向量（latent vector），在特征交叉时，使用两个特征隐向量的内积作为交叉特征的权重，而不是单一的权重。 通过引入特征隐向量的方式，直接把原先$n^2$级别的权重数量减低到了$nk$（$k$为隐向量维度，$n \gg k$）。在训练过程中，又可以通过优化二阶项的计算过程，使FM的训练复杂度进一步降低到$nk$级别，极大降低训练开销。 而且FM解决了上面多项式模型中的数据稀疏问题。从公式可以看出，所有包含“$x_i$的非零组合特征”（存在某个$j \neq i$，使得 $x_ix_j \neq 0$）的样本都可以用来学习隐向量$v_i$，这很大程度上避免了数据稀疏性造成的影响。 而且隐向量的引入也使得模型可以泛化到未曾出现过的特征组合中，兼顾了模型的记忆性和泛化性。 工程方面，FM同样可以用梯度下降进行学习的特点使其不失实时性和灵活性。相比之后深度学习模型复杂的网络结构，FM比较容易实现的inference过程也使其没有serving的难题,直到现在也常被应用于召回阶段。 3. GBDT + LRFM虽然综合效果很好，但是也只能够做二阶的特征交叉，如果要继续提高特征交叉的维度，不可避免的会发生组合爆炸和计算复杂度过高的情况。那么有没有其他的方法可以有效的处理高维特征组合和筛选的问题？ 2014年Facebook在论文中提出了一种级联结构的树模型，用于解决这个问题。 思想很简单: 先训练一个GBDT模型，每棵树自上而下，每个节点的分裂是一个自然的特征选择的过程，而多层下来自然进行了有效的特征组合，这样每个叶子节点就对应了树的一条路径，也代表不同的特征交叉组合，这样后面就可以将所有叶子节点进行编号作为新的特征，再结合原始特征，输入LR训练最终的模型。 树模型的特征组合可以不像FM那样局限于2阶交叉:例如，每棵树的深度为5，那么通过4次节点分裂，最终的叶节点实际上是进行了4阶特征组合后的结果。但也不能因此就说GBDT比FM的效果好，因为树模型也有自身的缺点，比如容易过拟合高维稀疏数据，比如不能并行速度慢、比如泛化性差等。 但GBDT+LR的重要的意义在于， 它提出了一种用模型自动进行特征工程、特征交叉的思想，某种意义上来说，之后深度学习的崛起，以及embedding的广泛应用，各种五花八门的网络结构，都是这一思想的延续和继承。 它还提出了一种级联结构，并且每个模型可以分步更新。树模型训练慢可以每天甚至每周进行一次大型训练，而LR模型可以在线上进行分钟级别甚至秒级别的实时更新。这也为后面的模型提供了一种可以借鉴的serving的想法。 小结从最早的人工规则排序，发展到人工进行特征组合的LR模型，再发展到自动进行二阶特征组合的FM模型，到后面高阶特征自动组合的LR+GBDT，这基本就是早期推荐系统排序模型的主脉络。 再往后，DNN模型的引入，标志着以深度学习为主的排序模型的崛起。纯粹的简单DNN模型本质上其实是在FM模型的特征Embedding化基础上，添加MLP隐层来进行隐式、特征非线性自动组合。 下面我们将重点介绍近年来独领风骚的深度学习模型。 二、深度模型2015年之后，以DNN为代表的一系列深度学习模型逐渐出现，个人认为他们可以归结为两类： 以Wide&amp;Deep为代表的一系列模型，它们的共同特点是自动学习从原始特征交叉组合新的高阶特征，它们的区别在于wide部分或者deep进行了部分改动。 以多任务学习为基础的联合训练模型，以ESMM、MMOE等模型为代表。 1. Wide&amp;Deep类模型这类模型的特点是双塔结构，即一边是以LR为代表的浅层模型（wide部分）用以学习低阶特征的表达，强调的是“记忆性”；另一边是以MLP为代表深层模型（deep部分），强调的是“泛化性”，deep部分的结构也大多可以分为下面几个模块： raw input-&gt;embedding: 把稀疏特征映射为低维稠密的embedding向量的过程。 input_layer：在这一层通常会对各特征的embedding会做一些聚合操作。 input_layer-&gt;output: 通常会用几层MLP的全连接框架连接到softmax作为输出层。 大多数模型在deep部分的区别只在于input_layer这一块，不同的模型在交叉方式（隐式/显式，元素级/向量级），或者特征之间的连接方式（concatenate/weighted sum/product/BI-interaction/attention 等）或显示特征交叉的阶数（二阶/高阶）之间会有所不同，以Wide&amp;Deep、(x)DeepFM、DCN、DIN为例，下面会简要介绍这几种模型。 这里先简单介绍下特征交互的方式： 一种是类似于MLP的方式，因其特殊的结构天然就具有学习高阶特征组合的能力，并且引入了一定的非线性；但至于怎么发生交互组合的，发生了多少阶的交叉，我们并不清楚，而且这种建模是元素级的（bit-wise），也就是说同一个域对应的embedding向量中的元素也会相互影响。因此我们说这种特征交叉方式是“隐式的、元素级的”。 另一种与之对应的是类似于DeepFM, xDeepFM的方式，在模型结构中，明确设计一些子网络或者子结构，可以对任意高阶的特征组合进行表征。以FM为例，就是向量级的方式明确对特征二阶组合进行建模，这种我们称之为“显式的、向量级的”。 Wide&amp;Deep 模型wide部分长处在于学习样本中的高频部分，“记忆性”好，对于样本中出现过的高频低阶特征能够用少量参数学习。但因为是LR模型，仍需人工进行特征交叉组合。 模型deep部分用于学习特征之间的高阶交叉组合关系，引入了“泛化性”。是一种隐式、元素级的特征交叉。 这种双塔框架结构的提出极大的促进了后续模型的发展 DeepFM Wide&amp;Deep框架固然强大，但由于wide部分是个LR模型，仍然需要人工特征工程。DeepFM模型将wide部分替换为FM，可以自动进行特征的二阶交叉。但它的局限也在于只有二阶交叉，而没有更高阶的交叉。 FM和deep部分共享embedding层，双边共同更新权重矩阵。这种share bottom的思想在后面模型中也很常见。 DCN想要得到任意高阶的特征交叉组合，而不仅仅是二阶，但是又要避免产生组合爆炸的维数灾难，导致网络参数过于庞大而无法学习，同时也会产生很多的无效交叉特征，需要设计一种有“压缩”能力的，并且足够高效的网络结构。DCN就是其中的一种： 向量交叉方式如下： 在保留了MLP结构的基础上，还额外引入了cross layer network，理论上可以表达任意高阶组合，同时每一层保留低阶组合，参数的向量化以及独特的网络结构也控制了模型的复杂度是线性增长的，不至于随着交叉阶数的增大而发生维度爆炸。 但在xDeepFM论文中指出,DCN的特征交叉是限定在一种特殊的形式上的，并且还是以bit-wise的方式构建的。 xDeepFMxDeepFM模型是自动构建交叉特征且能够端到端学习的集大成者，它有效的解决了DCN中提到的问题，实现自动学习显式的高阶特征交互，同时使得交互发生在向量级上。 模型设计了一种独特的CIN结构，经过外积、卷积等操作来提取特征的交叉。 并且可以通过控制CIN的层数来显式控制特征交叉的阶数，并且计算的时空复杂度是线性增长的，而不会出现组合爆炸导致参数爆炸而无法训练的情况。 集成的CIN和DNN两个模块能够帮助模型同时以显式和隐式，元素级和向量级的方式学习任意高阶的特征交互，并且保持线性复杂度。 DIN 特点在于拿到当前物品以及用户所有交互过物品的embedding，之后通过attention network，对每个兴趣表示赋予不同的权值，之后进行weighted sum，得到一个汇总的embedding作为针对当前物品的用户兴趣的表示，相当于增加了一步自动特征选择。 也反映了模型是服务于场景的这一理念。先观察到用户兴趣的多峰分布以及部分对应的数据特点，再因地制宜的提出合适的模型进行拟合。 小结 2. 多任务类模型推荐系统的多目标优化，是目前业界的主流之一，也是很多公司的研发现状。以我们花椒直播为例，可以优化的目标有点击、观看、送礼、评论、关注、转发等等。 多任务模型旨在平衡不同目标的相互影响，尽量能够做到所有指标同步上涨，即使不能，也要尽量做到在某个优化目标上涨的情况下，不拉低或者将尽量少拉低其它指标，力求达到全局最优的效果。 这里主要介绍ESMM和MMOE两个模型。 ESMM论文指出，完整的日志流程应该包括： $$impression-&gt;click-&gt;conversion$$ 样本空间如下图所示： 传统的CVR任务，只考虑从浏览到转化的过程，即 $$p(cvr) = p(conversion=1|impression)$$ 而本模型还考虑了点击的过程，并引入了浏览转化率（pCTCVR）的概念，即为在浏览的条件下既点击又转化的概率，即 $$p(conversion=1,click=1|impression) = p(click=1|impression) * p(conversion=1|click=1, impression)$$ 也就是下面的公式： 根据这个公式设计的模型如下图： 用两个任务共享底层embedding的方式，然后将各自的logit进行相乘来拟合pCTCVR的过程。 样本的构建为： 任务 正样本 负样本 pCTR 点击 未点击 pCTCVR 点击且转化 未转化 模型特点： 在全样本空间进行建模，避免了“样本选择偏差”问题，充分利用了业务数据。 共享底层embedding向量，因为推荐中转化率是很低的，对应的数据很少，这种共享特征表示的机制使CVR网络也能够从只曝光没点击的样本中更新embedding向量，有利于缓解CVR训练数据稀疏的问题。 ESMM中的子网络可以不止局限于本文中的MLP，可以随意替换成其他模型，也就是说本文为我们提供了一种可扩展的多任务模型架构。阿里后面还有一篇$ESM^2$的论文也是类似的套路。 MMOE 论文指出：一般的多任务模型结构如上图(a)所示，即对于不同的任务，底层的参数和网络结构是共享的，然后上层经过不同的神经网络得到对应任务的输出，缺点是模型的效果取决于任务的相关性，如果多任务之间关联较小，采用这种结构甚至会出现互相拖后腿的情况。 因此本论文提出了基于图(b)的OMOE和图(c)的MMOE两种结构，主要思路是每个任务有一个独立的expert中间网络，类似于“开关”的功能，通过模型学习，不同的任务可以从相同的底层embedding中提取到侧重点不同的特征，而不是完全共享底层，即达到了“各取所需”的效果，有点类似于上面提到的attention网络。 之后每个任务接各自的tower模型，得到logit，再和label一起计算loss，然后多目标的loss直接可以用类似weighted sum的方式结合起来组成总的loss。 小结推荐系统的多任务模型，虽然是排序模型的一大发展趋势，但多目标学习的难处在于，每个目标的样本比例是不同的，训练时如何融合loss、何时停止训练以及线上各目标的分数如何组合、A/B test如何衡量总体效果等方面，都要经过比较复杂的衡量和考虑，这些都还有很大的发展空间，需要我们来尝试。 排序模型在花椒直播中的实践近两年来，花椒直播紧跟业界潮流，在排序阶段进行了多种多样的尝试，如(GBDT+)LR，Wide&amp;Deep，(x)DeepFM，DIN，ESMM，MMOE等。下面以Wide&amp;Deep模型为例，简单介绍下我们的整个排序系统。 首先是离线部分，我们主要用spark/hdfs来处理、存储数据。主要包括用户数据、主播数据、实时数据、行为序列等等。下面是我们用到的部分特征： 类别 特征 用户画像 性别、年龄、机型、地域、看播/打赏/弹幕/转发/关注的统计、行为序列…… 主播画像 性别、年龄、地域、等级、类别、标签、频道、达人、时长、收礼、粉丝以及各种统计类排行…… 实时特征 实时看播、打赏、弹幕、热度、是否唱歌、是否跳舞、游戏精彩瞬间…… 生产好用户、主播画像后，还要生产一个基于用户主播交互数据的标签集，可以是多标签以便于多任务模型使用，比如用户浏览过哪些主播，是否发生了观看、打赏、评论、关注等行为，如果发生了，还可以把如看播时长等程度数据作为权重，以便后面进行加权训练时使用。 之后将标签集和画像join起来，就形成了一天的数据集，可以用多天的训练集共同组成最终的整体数据集来满足数据量和覆盖度的要求，这里要小心不要发生数据穿透。最终的数据集是T级别的，存储在HDFS上。在训练阶段，单机多卡的配置也满足不了速度上的要求，因此我们采用了360私有云的hbox分布式训练平台，来完成日常深度模型的训练。 下面是我们的模型结构图： 下面是我们部分模型的效果： 离线： 模型 AUC*100 FM 78.9 Wide&amp;Deep 84.5 DeepFM 84.7 线上：热门频道接入个性化推荐后人均观看时长涨幅&gt;80% 后记本文只是对业内近年来常用的模型进行的简单的介绍和总结，实际上每个模型除了其典型的结构外，还有许多非常珍贵的细节，比如公式推导，参数的选择，工程上的trick等等，这些建议大家还是要精读下相关模型论文。 并且要注意的是，没有“最好的模型”，只有“最适合的模型”，并不是说模型越fancy越复杂，线上效果就会越好。比如阿里提出了DIN模型，是因为工程师们首先发现了数据中的现象： 用户在浏览电商网站的过程中显示出的兴趣是十分多样性的，并且只有部分历史数据会影响到当次推荐的物品是否被点击，而不是所有的历史记录，即“多峰分布”、“部分激活”。 正是这种特定场景的需要，才使得阿里研发了DIN模型，来捕获用户兴趣的进化，取得了突破的效果。 所以做推荐正确的顺序应该是先有特定的“场景”，然后再基于用户行为和数据的特点，对应的开发适用于这个场景的模型；而不是先拍板一个模型，再去数据中进行试验，就本末倒置了。]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTR简介]]></title>
    <url>%2F2018%2F08%2F02%2FCTR%2F</url>
    <content type="text"><![CDATA[FM/FFM摘要在计算广告领域点击率CTR和转化率CVR是衡量广告流量的两个关键指标.准确的估计CTR、CVR对于提高流量的价值, 增加广告收入有重要的指导作用.预估CTR或CVR, 业界常用的方法有：–人工特征+LR–GBDT+LR–FM–FFM LR模型优缺点:1234优点：--算法简单, 易并行和工程化, 可以处理高纬度稀疏问题, 模型可解释性强, 通过ftrl算法方便实时对online模型进行更新缺点：--需要大量特征工程预先分析出有效的特征、特征组合, 从而去间接增强LR的非线性学习能力, 但又无法直接通过特征笛卡尔积解决, 只能依靠人工经验, 耗时耗力同时并不一定会带来效果提升 二阶多项式模型: 线性模型只考虑了单一特征对预测结果的影响, 没有考虑特征组合对结果的影响.常用的特征组合方式是多项式模型： 那么, 如何解决二次项参数的训练问题呢？矩阵分解提供了一种解决思路.在model-based的协同过滤中, 一个rating矩阵可以分解为user矩阵和item矩阵, 每个user和item都可以采用一个隐向量表示 FM模型: FM二次项化简过程如下： 最终, 采用SGD训练模型, 模型各个参数的梯度更新公式如下: FFM模型: FFM（Field-aware Factorization Machine）最初的概念来自Yu-Chin Juan（阮毓钦, 毕业于中国台湾大学, 现在美国Criteo工作）与其比赛队员, 是他们借鉴了来自Michael Jahrer的论文的field概念提出了FM的升级版模型.通过引入field的概念, FFM把相同性质的特征归于同一个field. FM在深度学习中的应用: 深度学习目前应用现状:–深度学习目前比较成熟的应用主要集中在：机器视觉、语音识别、自然语言处理这些课题上, 这些应用领域的共同特点是它们的数据集是连续的.–但在推荐系统、广告展示及搜索领域, 这些领域的数据集大部分是Multi-field Categorical类型的, 主要呈现以下特征：有多个域, 每个域上的数据以ID格式呈现.Multi-field Categorical Data与上述这些连续或是序列数据是有区别的, Multi-field Categorical Data会有多种不同的字段, 那这样就很难识别这些特征之间的关系, 需要消耗大量人力做一些高阶特征组合.–如果直接用One-Hot Binary进行编码, 那输入特征至少有一百万, 第一层至少需要500个节点, 那么第一层我们就需要训练5亿个参数, 那就需要20亿或是50亿的数据集, 而要获得如此大的数据集基本上是很困难的事情 因为上述原因, 我们需要将非常大的特征向量嵌入到低维向量空间中来减小模型复杂度, 而FM无疑是被业内公认为最有效的embedding model –第一部分仍然为Logistic Regression–第二部分是通过两两向量之间的点积来判断特征向量之间和目标变量之间的关系 我们就基于这个模型来考虑神经网络模型, 其实这个模型本质上就是一个三层网络： FM = LR+ embedding可以看成底层为特征维度为n的离散输入, 经过embedding层后, 对embedding层线性部分（LR）和非线性部分（特征交叉部分）累加后输出. FNN模型: 用FM算法对底层field进行embeddding, 在此基础上面建模就是FNN(Factorisation-machine supported Neural Networks)模型:–FNN假设每个field有且只有一个值为1, 其他均为0.x为原始输入的特征, 它是大规模离散稀疏的.它可以分成n个field, 每一个field中, 只有一个值为1, 其余都为0(即one hot) 网络结构如下: FNN具有以下几个特点： FM参数需要预训练:–FM部分的embedding需要预先进行训练, 所以FNN不是一个end-to-end模型. 无法拟合低阶特征:–FM得到的embedding向量直接concat连接之后作为MLP的输入去学习高阶特征表达, 最终的DNN输出作为ctr预估值.因此, FNN对低阶信息的表达比较有限. 每个field只有一个非零值的强假设:–FNN假设每个fileld只有一个值为非零值, 如果是稠密原始输入, 则FNN失去意义.对于一个fileld有几个非零值的情况, 例如用户标签可能有多个, 一般可以做average/sum/max等处理. FNN = LR + DEEP = LR + embedding + MLP PNN模型: FNN的embedding层直接concat连接后输出到MLP中去学习高阶特征. PNN, 全称为Product-based Neural Network, 认为在embedding输入到MLP之后学习的交叉特征表达并不充分, 提出了一种product layer的思想, 既基于乘法的运算来体现体征交叉的DNN网络结构, 如图所示. 对比FNN网络, PNN的区别在于中间多了一层Product Layer层, 除了Product layer不同, PNN和FNN的MLP结构是一样的:–Product Layer层由两部分组成, 左边z为embedding层的线性部分, 右边为embedding层的特征交叉部分.–这种product思想来源于, 在ctr预估中, 认为特征之间的关系更多是一种and“且”的关系, 而非add”加”的关系.例如, 性别为男且喜欢游戏的人群, 比起性别男和喜欢游戏的人群, 前者的组合比后者更能体现特征交叉的意义.–根据product的方式不同, 可以分为inner product(IPNN)(点乘)和outer product(OPNN)(矩阵乘法). Wide&amp;Deep模型: 前面介绍的两种变体DNN结构FNN和PNN, 都在embedding层对输入做处理后输入MLP, 让神经网络充分学习特征的高阶表达, deep部分是有了, 对高阶的特征学习表达较强, 但wide部分的表达是缺失的, 模型对于低阶特征的表达却比较有限.google在2016年提出了大名鼎鼎的wide&amp;Deep的结构正是解决了这样的问题.Wide&amp;deep结合了wide模型的优点和deep模型的优点, 网络结构如图所示, wide部分是LR模型, Deep部分是DNN模型. 在这个经典的wide&amp;deep模型中, google提出了两个概念, generalization（泛化性）和memory（记忆性）. Memory（记忆性）–wide部分长处在于学习样本中的高频部分, 优点是模型的记忆性好, 对于样本中出现过的高频低阶特征能够用少量参数学习；缺点是模型的泛化能力差, 例如对于没有见过的ID类特征, 模型学习能力较差. Generalization（泛化性）–deep部分长处在于学习样本中的长尾部分, 优点是泛化能力强, 对于少量出现过的样本甚至没有出现过的样本都能做出预测（非零的embedding向量）; 缺点是模型对于低阶特征的学习需要用较多参才能等同wide部分效果, 而且泛化能力强某种程度上也可能导致过拟合出现bad case. 除此之外, wide&amp;deep模型还有如下特点: 人工特征工程–LR部分的特征, 仍然需要人工设计才能保证一个不错的效果.因为LR部分是直接作为最终预测的一部分, 如果作为wide部分的LR特征工程做的不够完善, 将影响整个wide&amp;deep的模型精度. 联合训练–模型是end-to-end结构, wide部分和deep部分是联合训练的. embedding层deep部分单独占有–LR部分直接作为最后输出, 因此embedding层是deep部分独有的. wide&amp;deep = LR + embedding + MLP 9.DeepFM模型: google提出的wide&amp;deep框架固然强大, 但由于wide部分是个LR模型, 仍然需要人工特征工程. 但wide&amp;deep给整个学术界和工业界提供了一种框架思想.基于这种思想, 华为诺亚方舟团队结合FM相比LR的特征交叉的功能, 将wide&amp;deep部分的LR部分替换成FM来避免人工特征工程, 于是有了deepFM, 网络结构如图所示: 比起wide&amp;deep的LR部分, deeFM采用FM作为wide部分的输出, FM部分如图所示: ps: 注意图中的红线部分, 是权重为1的连接, 而不是像MLP中需要学习的权重连接, 最后wide(fm)部分的输出仅仅是一个数和deep(dnn)部分合到一起做sigmoid !!! 除此之外, deepFM还有如下特点： 低阶特征表达–wide部分取代WDL的LR, 比FNN和PNN相比能捕捉低阶特征信息. embedding层共享–wide&amp;deep部分的embedding层得需要针对deep部分单独设计；而在deepFM中, FM和DEEP部分共享embedding层, FM训练得到的参数既作为wide部分的输出, 也作为DNN部分的输入. end-end训练–embedding和网络权重联合训练, 无需预训练和单独训练 deepFM = FM + embedding + DNN 同样是利用FM做embedding降维, 下面是几种模型的比较: embedding vector这层的融合是深度学习模型改造最多的地方，该层是进入深度学习模型的输入层，embedding融合的质量将影响DNN模型学习的好坏. 常用的融合方式有以下几种： 模型超参数设置:12341、dropout,业内认为0.5是比较好的值2、最佳的隐层层数, 隐层层数并不是越多越好, 层数过多的模型会出现过拟合效应, 这个隐层数是跟数据集大小相关, 一般而言数据集越大所需要的隐层就越多, 我们这里模型显示的最佳隐层是3层3、神经网络不同层级节点数目的分布, 一下有四种不同的层级节点分布形态, 结果发现constant 和 diamond 这两种形态的表现效果比较好, increasing形态效果最差, 说明我们不应该在第一层就过度压缩特征向量4、不同隐层节点的Activation Functions的效果, 结果发现tanh 和 relu明显优于sigmoid Reference:1234FM原理推导: https://www.cnblogs.com/pinard/p/6370127.html深入FFM原理与实践: https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.htmlwide&amp;deep: https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.htmlGBDT+LR: https://blog.csdn.net/shine19930820/article/details/71713680]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
</search>
