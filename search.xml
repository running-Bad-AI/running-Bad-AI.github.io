<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度兴趣网络(DIN,Deep Interest Network)]]></title>
    <url>%2F2018%2F08%2F02%2FDIN%2F</url>
    <content type="text"><![CDATA[Deep Interest Network: 阿里深度兴趣网络摘要深度兴趣网络(DIN,Deep Interest Network):该方法由阿里妈妈的精准定向检索及基础算法团队提出, 针对电子商务领域(e-commerce industry), 充分利用/挖掘用户历史行为数据中的信息来提高CTR预估的性能. contributions/key words: 1234-diversity -local activation (attention) -mini-batch aware regularizer -data adaptive activation function (dice) old methods: 1234567常见的算法, 比如 Wide&amp;Deep, DeepFM等, 通常流程是: Sparse Features -&gt; Embedding Vector (-&gt; pooling layer) -&gt; MLPs -&gt; Sigmoid -&gt; Output 通过Embedding层, 将高维离散特征转换为固定长度的连续特征, 然后通过多个全联接层, 学习非线性, 最后通过一个sigmoid函数转化为0-1值, 代表点击的概率优点：通过神经网络可以拟合高阶的非线性关系, 同时减少了人工特征的工作量. 缺点：在对用户历史行为数据进行处理时, 每个用户的历史点击个数是不相等的, 包含了许多兴趣信息, 我们要把它们编码成一个固定长的向量, 需要做pooling (sum or average), 会损失信息 DIN: 12Diversity：用户在浏览电商网站的过程中显示出的兴趣是十分多样性的. Local activation: 由于用户兴趣的多样性, 只有部分历史数据会影响到当次推荐的物品是否被点击, 而不是所有的历史记录. Features: 12只有用户行为特征是multi-hot, 即多值离散. 没有人工组合特征, 会在dnn中自己学习. Architecture: 12Activation Unit实现Attention机制, 对Local Activation建模. Pooling(weighted sum)对Diversity建模, 直接sum体现不出差异多样性, 加权可以. 123456789其中： Vi表示behavior id的嵌入向量, 比如good_id, shop_id等; Vu是所有behavior ids的加权和, 表示的是用户兴趣; Va是候选广告的嵌入向量; wi是候选广告影响着每个behavior id的权重, 也就是Local Activation; wi通过Activation Unit计算得出, 这一块用函数去拟合, 表示为g(Vi,Va). 在实际实现中, 权重用激活函数Dice的输出来表示, 输入是Vi和Va. 优点：针对不同的候选广告, 用户的兴趣向量是不同的, 而不像单纯的sum pooling兴趣永远是不变的. DICE: data adaptive activation function: 类似relu + BN 的组合: 123456优点: 1.将数据做标准化估计, 统一所有维度的量纲, 也是个非常重要的技巧, 有点类似BN.--在训练过程中, 分别是当次batch的均值和方差.--在Test时, 这里的E[s]和Var[s]用的是moving_average.2.可以发现当E[s]=0, Var[s]=0 时, Dice几乎等同于PRELU.3.ϵ is a small constant which is set to be 10−8 in our practice. GAUC: 计算了用户级别的AUC, 在将其按展示次数进行加权, 消除了用户偏差对模型评价的影响, 更准确地描述了模型对于每个用户的表现效果 1234567891011121314151617AUC意义: AUC值越大, 当前的分类算法越有可能将正样本排在负样本前面, 即能够更好的分类.首先要肯定的是, AUC是要分用户看的, 我们的模型的预测结果, 只要能够保证对每个用户来说, 他想要的结果排在前面就好了.Example 1：假设有两个用户A和B, 每个用户都有10个商品, 10个商品中有5个是正样本.我们分别用TA, TB, FA, FB来表示两个用户的正样本和负样本.假设模型预测的结果大小排序依次为TA, FA, TB, FB1）如果把两个用户的结果混起来看, AUC并不是很高, 因为有5个正样本排在了后面.2）但是分开看的话, 每个用户的正样本都排在了负样本之前, AUC应该是1.显然, 分开看更容易体现模型的效果, 这样消除了用户本身的差异.Example 2：还有一种差异是用户的展示次数或者点击数, 这种差异同样需要消除.如果一个用户有1个正样本, 10个负样本; 另一个用户有5个正样本, 50个负样本.那么GAUC的计算, 不仅将每个用户的AUC分开计算, 同时根据用户的展示数或者点击数来对每个用户的AUC进行加权处理, 进一步消除了用户偏差对模型的影响.通过实验证明, GAUC确实是一个更加合理的评价指标. mini-batch aware Regularization （MBA）: 1234567891011场景:CTR中输入稀疏而且维度高, 通常的做法是加入L1、L2、Dropout等防止过拟合.--但是论文中尝试后效果都不是很好：用户数据符合长尾定律(long-tail law), 也就是说很多的feature id只出现了几次, 而一小部分feature id出现很多次.--这在训练过程中增加了很多噪声, 并且加重了过拟合.对于这个问题一个简单的处理办法就是：--直接去掉出现次数比较少的feature id.但是这样就人为的丢掉了一些信息, 导致模型更加容易过拟合.--同时阈值的设定作为一个新的超参数, 也是需要大量的实验来选择的.MBA的优点:1.频率自适应 2.每次迭代只更新非0部分权重,减少计算量 原理: 效果: 12利用候选的广告, 反向激活历史兴趣, 不同的历史兴趣爱好对于当前候选广告的权重不同, 做到了local activation.--可以看到, 对于候选的广告是一件衣服的时候, 用户历史行为中跟衣服相关性越高的在attention后获得的权重越高（即越能描述用户对这个广告的兴趣）, 而非衣服的部分, 权重较低. 123以上面年轻妈妈为例, 选取9个类别各100条商品, 作为candidate ad输入模型, 得到每件商品的embedding vector以及预测得分.进行可视化如上图, 红色表示得分最高, 蓝色表示得分最低,可以发现：--用户的兴趣分布有多个峰.且DIN有较好的聚类效果. summary: 12345671.用户有多个兴趣爱好, 即访问了多个good_id, shop_id.为了降低维度并使得商品店铺间的算术运算有意义, 我们先对其进行Embedding嵌入.那么我们如何对用户多种多样的兴趣建模？--使用Pooling对Embedding Vector求和或者求平均.同时这也解决了不同用户输入长度不同的问题, 得到了一个固定长度的向量.这个向量就是用户表示, 是用户兴趣的代表.2.但是, 直接求sum或average(相当于平均权重, 没有侧重)损失了很多信息.--所以稍加改进, 针对不同的behavior赋予不同的权重, 这个权重是由用户历史behavior和当前候选广告共同决定的.这就是Attention机制, 实现了Local Activation.3.DIN使用activation unit来捕获local activation的特征, 使用weighted sum pooling来实现diversity结构.4.在模型学习优化上, DIN提出了Dice激活函数、MBA(自适应正则化) , 显著的提升了模型性能与收敛速度. Reference: 123456Github: https://github.com/running-Bad-AI/DeepInterestNetworkDeep Interest Network for Click-Through Rate PredictionLearning piece-wise linear models from large scale data for ad click predictionhttps://www.leiphone.com/news/201707/t0AT4sIgyWS2QWVU.htmlhttps://www.leiphone.com/news/201706/pDfOAoMYp8mqNKEC.html盖坤的分享视频 http://www.itdks.com/dakalive/detail/3166]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTR常用模型]]></title>
    <url>%2F2018%2F08%2F02%2FCTR%2F</url>
    <content type="text"><![CDATA[FM/FFM摘要在计算广告领域点击率CTR和转化率CVR是衡量广告流量的两个关键指标.准确的估计CTR、CVR对于提高流量的价值, 增加广告收入有重要的指导作用.预估CTR或CVR, 业界常用的方法有：–人工特征+LR–GBDT+LR–FM–FFM LR模型优缺点:1234优点：--算法简单, 易并行和工程化, 可以处理高纬度稀疏问题, 模型可解释性强, 通过ftrl算法方便实时对online模型进行更新缺点：--需要大量特征工程预先分析出有效的特征、特征组合, 从而去间接增强LR的非线性学习能力, 但又无法直接通过特征笛卡尔积解决, 只能依靠人工经验, 耗时耗力同时并不一定会带来效果提升 二阶多项式模型: 线性模型只考虑了单一特征对预测结果的影响, 没有考虑特征组合对结果的影响.常用的特征组合方式是多项式模型： 那么, 如何解决二次项参数的训练问题呢？矩阵分解提供了一种解决思路.在model-based的协同过滤中, 一个rating矩阵可以分解为user矩阵和item矩阵, 每个user和item都可以采用一个隐向量表示 FM模型: FM二次项化简过程如下： 最终, 采用SGD训练模型, 模型各个参数的梯度更新公式如下: FFM模型: FFM（Field-aware Factorization Machine）最初的概念来自Yu-Chin Juan（阮毓钦, 毕业于中国台湾大学, 现在美国Criteo工作）与其比赛队员, 是他们借鉴了来自Michael Jahrer的论文的field概念提出了FM的升级版模型.通过引入field的概念, FFM把相同性质的特征归于同一个field. FM在深度学习中的应用: 深度学习目前应用现状:–深度学习目前比较成熟的应用主要集中在：机器视觉、语音识别、自然语言处理这些课题上, 这些应用领域的共同特点是它们的数据集是连续的.–但在推荐系统、广告展示及搜索领域, 这些领域的数据集大部分是Multi-field Categorical类型的, 主要呈现以下特征：有多个域, 每个域上的数据以ID格式呈现.Multi-field Categorical Data与上述这些连续或是序列数据是有区别的, Multi-field Categorical Data会有多种不同的字段, 那这样就很难识别这些特征之间的关系, 需要消耗大量人力做一些高阶特征组合.–如果直接用One-Hot Binary进行编码, 那输入特征至少有一百万, 第一层至少需要500个节点, 那么第一层我们就需要训练5亿个参数, 那就需要20亿或是50亿的数据集, 而要获得如此大的数据集基本上是很困难的事情 因为上述原因, 我们需要将非常大的特征向量嵌入到低维向量空间中来减小模型复杂度, 而FM无疑是被业内公认为最有效的embedding model –第一部分仍然为Logistic Regression–第二部分是通过两两向量之间的点积来判断特征向量之间和目标变量之间的关系 我们就基于这个模型来考虑神经网络模型, 其实这个模型本质上就是一个三层网络： FM = LR+ embedding可以看成底层为特征维度为n的离散输入, 经过embedding层后, 对embedding层线性部分（LR）和非线性部分（特征交叉部分）累加后输出. FNN模型: 用FM算法对底层field进行embeddding, 在此基础上面建模就是FNN(Factorisation-machine supported Neural Networks)模型:–FNN假设每个field有且只有一个值为1, 其他均为0.x为原始输入的特征, 它是大规模离散稀疏的.它可以分成n个field, 每一个field中, 只有一个值为1, 其余都为0(即one hot) 网络结构如下: FNN具有以下几个特点： FM参数需要预训练:–FM部分的embedding需要预先进行训练, 所以FNN不是一个end-to-end模型. 无法拟合低阶特征:–FM得到的embedding向量直接concat连接之后作为MLP的输入去学习高阶特征表达, 最终的DNN输出作为ctr预估值.因此, FNN对低阶信息的表达比较有限. 每个field只有一个非零值的强假设:–FNN假设每个fileld只有一个值为非零值, 如果是稠密原始输入, 则FNN失去意义.对于一个fileld有几个非零值的情况, 例如用户标签可能有多个, 一般可以做average/sum/max等处理. FNN = LR + DEEP = LR + embedding + MLP PNN模型: FNN的embedding层直接concat连接后输出到MLP中去学习高阶特征. PNN, 全称为Product-based Neural Network, 认为在embedding输入到MLP之后学习的交叉特征表达并不充分, 提出了一种product layer的思想, 既基于乘法的运算来体现体征交叉的DNN网络结构, 如图所示. 对比FNN网络, PNN的区别在于中间多了一层Product Layer层, 除了Product layer不同, PNN和FNN的MLP结构是一样的:–Product Layer层由两部分组成, 左边z为embedding层的线性部分, 右边为embedding层的特征交叉部分.–这种product思想来源于, 在ctr预估中, 认为特征之间的关系更多是一种and“且”的关系, 而非add”加”的关系.例如, 性别为男且喜欢游戏的人群, 比起性别男和喜欢游戏的人群, 前者的组合比后者更能体现特征交叉的意义.–根据product的方式不同, 可以分为inner product(IPNN)(点乘)和outer product(OPNN)(矩阵乘法). Wide&amp;Deep模型: 前面介绍的两种变体DNN结构FNN和PNN, 都在embedding层对输入做处理后输入MLP, 让神经网络充分学习特征的高阶表达, deep部分是有了, 对高阶的特征学习表达较强, 但wide部分的表达是缺失的, 模型对于低阶特征的表达却比较有限.google在2016年提出了大名鼎鼎的wide&amp;Deep的结构正是解决了这样的问题.Wide&amp;deep结合了wide模型的优点和deep模型的优点, 网络结构如图所示, wide部分是LR模型, Deep部分是DNN模型. 在这个经典的wide&amp;deep模型中, google提出了两个概念, generalization（泛化性）和memory（记忆性）. Memory（记忆性）–wide部分长处在于学习样本中的高频部分, 优点是模型的记忆性好, 对于样本中出现过的高频低阶特征能够用少量参数学习；缺点是模型的泛化能力差, 例如对于没有见过的ID类特征, 模型学习能力较差. Generalization（泛化性）–deep部分长处在于学习样本中的长尾部分, 优点是泛化能力强, 对于少量出现过的样本甚至没有出现过的样本都能做出预测（非零的embedding向量）; 缺点是模型对于低阶特征的学习需要用较多参才能等同wide部分效果, 而且泛化能力强某种程度上也可能导致过拟合出现bad case. 除此之外, wide&amp;deep模型还有如下特点: 人工特征工程–LR部分的特征, 仍然需要人工设计才能保证一个不错的效果.因为LR部分是直接作为最终预测的一部分, 如果作为wide部分的LR特征工程做的不够完善, 将影响整个wide&amp;deep的模型精度. 联合训练–模型是end-to-end结构, wide部分和deep部分是联合训练的. embedding层deep部分单独占有–LR部分直接作为最后输出, 因此embedding层是deep部分独有的. wide&amp;deep = LR + embedding + MLP 9.DeepFM模型: google提出的wide&amp;deep框架固然强大, 但由于wide部分是个LR模型, 仍然需要人工特征工程. 但wide&amp;deep给整个学术界和工业界提供了一种框架思想.基于这种思想, 华为诺亚方舟团队结合FM相比LR的特征交叉的功能, 将wide&amp;deep部分的LR部分替换成FM来避免人工特征工程, 于是有了deepFM, 网络结构如图所示: 比起wide&amp;deep的LR部分, deeFM采用FM作为wide部分的输出, FM部分如图所示: ps: 注意图中的红线部分, 是权重为1的连接, 而不是像MLP中需要学习的权重连接, 最后wide(fm)部分的输出仅仅是一个数和deep(dnn)部分合到一起做sigmoid !!! 除此之外, deepFM还有如下特点： 低阶特征表达–wide部分取代WDL的LR, 比FNN和PNN相比能捕捉低阶特征信息. embedding层共享–wide&amp;deep部分的embedding层得需要针对deep部分单独设计；而在deepFM中, FM和DEEP部分共享embedding层, FM训练得到的参数既作为wide部分的输出, 也作为DNN部分的输入. end-end训练–embedding和网络权重联合训练, 无需预训练和单独训练 deepFM = FM + embedding + DNN 同样是利用FM做embedding降维, 下面是几种模型的比较: embedding vector这层的融合是深度学习模型改造最多的地方，该层是进入深度学习模型的输入层，embedding融合的质量将影响DNN模型学习的好坏. 常用的融合方式有以下几种： 模型超参数设置:12341、dropout,业内认为0.5是比较好的值2、最佳的隐层层数, 隐层层数并不是越多越好, 层数过多的模型会出现过拟合效应, 这个隐层数是跟数据集大小相关, 一般而言数据集越大所需要的隐层就越多, 我们这里模型显示的最佳隐层是3层3、神经网络不同层级节点数目的分布, 一下有四种不同的层级节点分布形态, 结果发现constant 和 diamond 这两种形态的表现效果比较好, increasing形态效果最差, 说明我们不应该在第一层就过度压缩特征向量4、不同隐层节点的Activation Functions的效果, 结果发现tanh 和 relu明显优于sigmoid Reference:1234FM原理推导: https://www.cnblogs.com/pinard/p/6370127.html深入FFM原理与实践: https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.htmlwide&amp;deep: https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.htmlGBDT+LR: https://blog.csdn.net/shine19930820/article/details/71713680]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反垃圾]]></title>
    <url>%2F2018%2F08%2F02%2F3%2F</url>
    <content type="text"><![CDATA[文本反垃圾文本反垃圾场景文本反垃圾是网络社区应用非常常见的任务。因为各种利益关系，网络社区通常都难以避免地会涌入大量骚扰、色情、诈骗等垃圾信息，扰乱社区秩序，伤害用户体验。这些信息往往隐晦，多变，传统规则系统如正则表达式匹配关键词难以应对。通常情况下，文本反垃圾离不开用户行为分析，本章只针对文本内容部分进行讨论。 为了躲避平台监测，垃圾文本常常会使用火星文等方式对关键词进行隐藏。例如： 123渴望 兂 极限 激情 恠 燃烧 加 涐 嶶 信 lovexxxx521亲爱 的 看 頭潒 约私人 企鹅 ⓧⓧⓧ㊆㊆⑧⑧⑧ 给 你 爽 你 懂 的 垃圾文本通常还会备有多个联系方式进行用户导流。识别异常联系方式是反垃圾的一项重要工作，但是传统的识别方法依赖大量策略，攻防压力大，也容易被突破。例如： 123自啪 试平 n 罗辽 婊研 危性 xxxx447自啪 试平 n 罗辽 婊研 危性 xxxxx11118自啪 试平 n 罗辽 婊研 危性 xxxx2323 在这个实例中，我们将使用TensorLayer来训练一个垃圾文本分类器，并介绍如何通过TensorFlow Serving来提供高性能服务，实现产品化部署。这个分类器将解决以上几个难题，我们不再担心垃圾文本有多么隐晦，也不再关心它们用的哪国语言或有多少种联系方式。 第一步，训练词向量，相关代码在word2vec文件夹，执行步骤见word2vec/README.md。 第二步，训练分类器，相关代码在network文件夹，执行步骤见network/README.md。 第三步，与TensorFlow Serving交互，客户端代码在serving文件夹。在这个实例中，我们将使用TensorLayer来训练一个垃圾文本分类器，并介绍如何通过TensorFlow Serving来提供高性能服务，实现产品化部署。这个分类器将解决以上几个难题，我们不再担心垃圾文本有多么隐晦，也不再关心它们用的哪国语言或有多少种联系方式。]]></content>
      <categories>
        <category>tech/反垃圾</category>
      </categories>
      <tags>
        <tag>反垃圾</tag>
        <tag>诗词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4]]></title>
    <url>%2F2018%2F08%2F02%2F4%2F</url>
    <content type="text"><![CDATA[第4篇秋风清，秋月明， 落叶聚还散，寒鸦栖复惊。 第1节相思相见知何日？此时此夜难为情！ ##]]></content>
      <categories>
        <category>爱好/诗词</category>
      </categories>
      <tags>
        <tag>诗词</tag>
      </tags>
  </entry>
</search>
